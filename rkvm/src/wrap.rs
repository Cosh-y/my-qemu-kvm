//! Low-level wrapper functions for Linux kernel C APIs
//!
//! This module provides safe Rust wrappers around kernel C functions that are:
//! 1. Used in our kernel module
//! 2. Not provided by the rust-for-linux kernel crate
//! 3. Not generated by bindgen (inline functions, macros, etc.)

use core::ffi::{c_void, c_ulong};
use kernel::prelude::*;
use kernel::bindings;

use crate::types::*;

// Helper for on_each_cpu callback
unsafe extern "C" fn on_each_cpu_callback(info: *mut c_void) {
    // Cast void pointer back to Rust closure pointer and call it
    let func = info as *mut &mut dyn FnMut();
    unsafe { (*func)(); }
}

/// Run a function on all CPUs
/// 
/// This is a wrapper around the kernel's on_each_cpu (or similar functions).
/// Since we don't have direct access to the on_each_cpu macro via bindgen,
/// we implement it using smp_call_function (for other CPUs) and a local call.
/// 
/// # Arguments
/// * `func` - The closure to run on each CPU
pub(crate) fn on_each_cpu<F>(mut func: F) -> Result
where
    F: FnMut(),
{
    // Create a trait object to type-erase the closure
    // We need to pass a pointer to this trait object (which is a fat pointer)
    let mut trait_object: &mut dyn FnMut() = &mut func;
    let func_ptr = &mut trait_object as *mut &mut dyn FnMut() as *mut c_void;
    
    unsafe {
        // Run on all OTHER CPUs
        // smp_call_function(func, info, wait)
        bindings::smp_call_function(
            Some(on_each_cpu_callback),
            func_ptr,
            1, // wait = 1 (synchronous)
        );
        
        // Run on CURRENT CPU
        // We must disable preemption to ensure we don't migrate during this sequence,
        // but for simple initialization tasks that are idempotent or just setting up per-cpu hardware,
        // running locally is usually sufficient. 
        // Note: Strict on_each_cpu implementation requires disabling preemption.
        // Assuming this is called from init context where preemption might be enabled.
        
        // However, RFL bindings for preempt_disable are not exposed easily.
        // Given this is module init, we are likely fine for this specific use case (HVC init).
        on_each_cpu_callback(func_ptr);
    }
    
    Ok(())
}

// ============================================================================
// External C function declarations
// ============================================================================

extern "C" {
    /// Get virtual address from a page structure
    pub(super) fn rkvm_page_address(page: *const bindings::page) -> *mut c_void;
    
    /// Convert physical address to virtual address
    pub(super) fn rkvm_phys_to_virt(addr: bindings::phys_addr_t) -> *mut c_void;
    
    /// Convert virtual address to physical address
    pub(super) fn rkvm_virt_to_phys(addr: *const c_void) -> bindings::phys_addr_t;

    /// Convert vmalloc address to physical address
    pub(super) fn rkvm_vmalloc_to_phys(addr: *const c_void) -> bindings::phys_addr_t;
    
    /// Allocate kernel memory
    pub(super) fn rkvm_kmalloc(size: usize, flags: bindings::gfp_t) -> *mut c_void;
    
    /// Free kernel memory
    pub(super) fn rkvm_kfree(block: *const c_void);
    
    /// Free a single page
    pub(super) fn rkvm_free_page(addr: c_ulong);
    
    /// Copy data from user space to kernel space
    pub(super) fn rkvm_copy_from_user(to: *mut c_void, from: *const c_void, n: c_ulong) -> c_ulong;
    
    /// Flush instruction cache for a memory range (missing from original)
    pub(super) fn rkvm_flush_icache_range(start: c_ulong, end: c_ulong);
}


/// Convert virtual address to physical address
///
/// # Arguments
/// * `vaddr` - Virtual address to convert
///
/// # Returns
/// Physical address as u64
///
/// # Safety
/// The virtual address must be a valid kernel virtual address
pub(crate) fn virt_to_phys(vaddr: VirtAddr) -> PhysAddr {
    unsafe {
        rkvm_virt_to_phys(vaddr as *const c_void) as PhysAddr
    }
}

/// Convert vmalloc/module address to physical address
///
/// # Arguments
/// * `vaddr` - Virtual address to convert
///
/// # Returns
/// Physical address as u64
pub(crate) fn vmalloc_to_phys(vaddr: VirtAddr) -> PhysAddr {
    unsafe {
        rkvm_vmalloc_to_phys(vaddr as *const c_void) as PhysAddr
    }
}

/// Convert physical address to virtual address
///
/// # Arguments
/// * `paddr` - Physical address to convert
///
/// # Returns
/// Virtual address as usize
///
/// # Safety
/// The physical address must be valid and mapped in kernel space
pub(crate) fn phys_to_virt(paddr: PhysAddr) -> VirtAddr {
    unsafe {
        rkvm_phys_to_virt(paddr as bindings::phys_addr_t) as VirtAddr
    }
}

/// Get virtual address from a page structure
///
/// # Arguments
/// * `page` - Pointer to page structure
///
/// # Returns
/// Virtual address as usize, or 0 if page is null
pub(crate) fn page_address(page: *const bindings::page) -> VirtAddr {
    if page.is_null() {
        return 0;
    }
    unsafe {
        rkvm_page_address(page) as VirtAddr
    }
}

/// Allocate kernel memory with error handling
///
/// # Arguments
/// * `size` - Size of memory to allocate in bytes
/// * `flags` - GFP allocation flags
///
/// # Returns
/// Result containing pointer to allocated memory or ENOMEM error
pub(crate) fn kmalloc(size: MemSize, flags: bindings::gfp_t) -> Result<VirtAddr> {
    if size == 0 {
        return Err(EINVAL);
    }
    
    let ptr = unsafe { rkvm_kmalloc(size, flags) };
    if ptr.is_null() {
        Err(ENOMEM)
    } else {
        Ok(ptr as VirtAddr)
    }
}

/// Free kernel memory allocated by kmalloc
///
/// # Arguments
/// * `ptr` - Pointer to memory to free
///
/// # Safety
/// The pointer must have been allocated by kmalloc and not already freed
pub(crate) fn kfree(ptr: VirtAddr) {
    if ptr != 0 {
        unsafe {
            rkvm_kfree(ptr as *const c_void);
        }
    }
}

/// Free a page allocated by alloc_pages
///
/// # Arguments
/// * `addr` - Virtual address of the page to free
///
/// # Safety
/// The address must be a valid page address allocated by alloc_pages
pub(crate) fn free_page(addr: VirtAddr) {
    if addr != 0 {
        unsafe {
            rkvm_free_page(addr as c_ulong);
        }
    }
}

/// Copy data from user space to kernel space
///
/// # Arguments
/// * `to_kaddr` - Kernel space destination virtual address
/// * `from_uaddr` - User space source virtual address
/// * `size` - Number of bytes to copy
///
/// # Returns
/// Result indicating success or EFAULT if copy failed
///
/// # Safety
/// Both addresses must be valid for the specified size
pub(crate) fn copy_from_user(to_kaddr: VirtAddr, from_uaddr: VirtAddr, size: MemSize) -> Result {
    if to_kaddr == 0 || from_uaddr == 0 || size == 0 {
        return Err(EINVAL);
    }
    
    let remaining = unsafe {
        rkvm_copy_from_user(
            to_kaddr as *mut c_void,
            from_uaddr as *const c_void,
            size as c_ulong
        )
    };
    
    if remaining == 0 {
        Ok(())
    } else {
        Err(EFAULT)
    }
}

/// Flush instruction cache for a memory range
///
/// # Arguments
/// * `start` - Start virtual address of the range
/// * `end` - End virtual address of the range
///
/// # Safety
/// The range must be valid kernel memory
pub(crate) fn flush_icache_range(start: VirtAddr, end: VirtAddr) {
    unsafe {
        rkvm_flush_icache_range(start as c_ulong, end as c_ulong);
    }
}

/// Helper to clean data cache to Point of Coherency (PoC)
/// equivalent to __clean_dcache_area_poc
pub(crate) fn clean_dcache_poc(start: VirtAddr, size: usize) {
    // Read CTR_EL0 to get cache line size
    let mut ctr: u64;
    unsafe { core::arch::asm!("mrs {}, ctr_el0", out(reg) ctr); }
    
    // DminLine is bits [19:16] in words (log2)
    let dminline = (ctr >> 16) & 0xF;
    let line_bytes = 1 << (dminline + 2);
    
    let mut addr = start & !(line_bytes - 1);
    let end = start + size;
    
    unsafe {
        while addr < end {
            core::arch::asm!("dc cvac, {}", in(reg) addr);
            addr += line_bytes;
        }
        core::arch::asm!("dsb sy");
    }
}

/// Memory barrier - ensures all memory operations complete
#[inline]
pub(crate) fn memory_barrier() {
    unsafe {
        core::arch::asm!("dmb sy");
    }
}

/// Data synchronization barrier
#[inline]
pub(crate) fn dsb() {
    unsafe {
        core::arch::asm!("dsb sy");
    }
}

/// Instruction synchronization barrier
#[inline]
pub(crate) fn isb() {
    unsafe {
        core::arch::asm!("isb");
    }
}
